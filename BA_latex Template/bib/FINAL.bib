Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Yu2008a,
abstract = {The growth of email users has resulted in the dramatic increasing of the spam emails during the past few years. In this paper, four machine learning algorithms, which are Na{\"{i}}ve Bayesian (NB), neural network (NN), support vector machine (SVM) and relevance vector machine (RVM), are proposed for spam classification. An empirical evaluation for them on the benchmark spam filtering corpora is presented. The experiments are performed based on different training set size and extracted feature size. Experimental results show that NN classifier is unsuitable for using alone as a spam rejection tool. Generally, the performances of SVM and RVM classifiers are obviously superior to NB classifier. Compared with SVM, RVM is shown to provide the similar classification result with less relevance vectors and much faster testing time. Despite the slower learning procedure, RVM is more suitable than SVM for spam classification in terms of the applications that require low complexity.},
author = {Yu, Bo and Xu, Zong-ben},
doi = {10.1016/j.knosys.2008.01.001},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2008/Yu, Xu - 2008 - A comparative study for content-based dynamic spam classification using four machine learning algorithms.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {naÄ±,neural network,relevance vector machine,spam classification,support vector machine,ve bayesian},
number = {4},
pages = {355--362},
title = {{A comparative study for content-based dynamic spam classification using four machine learning algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705108000026},
volume = {21},
year = {2008}
}
@book{Agresti2002,
abstract = {Principles of Stratigraphy reafferms the vital importance of stratigraphy to the earth sciences, and introduces the undergraduate to its key elements in a ...},
author = {Agresti, Alan},
booktitle = {John Wiley {\&} Sons, INC},
doi = {10.1037/023990},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2002/(Wiley Series in Probability and Statistics) Alan Agresti(auth.)-Categorical Data Analysis, Second Edition-John Wiley {\&} Sons, Inc.(2002).pdf:pdf},
isbn = {0123266505},
issn = {01232665},
pages = {732},
pmid = {393307},
publisher = {John Wiley {\&} Sons, INC},
title = {{Categorical Data Analysis: Second edition}},
url = {http://www.bcin.ca/Interface/openbcin.cgi?submit=submit{\&}Chinkey=127536},
year = {2002}
}
@article{Mikolov,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2013/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {0003-6951},
journal = {Advances in neural information processing systems},
month = {oct},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://www.crossref.org/deleted{\_}DOI.html http://arxiv.org/abs/1310.4546},
volume = {1},
year = {2013}
}
@book{Manning2008a,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher D. and Prabhakar, Raghavan and Schutze, Hinrich},
booktitle = {PhD Proposal},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2008/Introduction-to-information-retrieval.pdf:pdf},
isbn = {978-0-511-41405-3},
issn = {1098-6596},
keywords = {icle},
pages = {139--161},
pmid = {25246403},
title = {{Introduction to Information Retrieval}},
volume = {1},
year = {2008}
}
@article{Doan2017,
author = {Doan, Tri and Kalita, Jugal},
doi = {10.1109/CCWC.2017.7868366},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2017/10.1109@CCWC.2017.7868366.pdf:pdf},
isbn = {9781509042289},
journal = {2017 IEEE 7th Annual Computing and Communication Workshop and Conference, CCWC 2017},
keywords = {Open set,closed set assumption,incremental learning,text classification},
title = {{Overcoming the challenge for text classification in the open world}},
year = {2017}
}
@article{Nigam2000,
abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.},
author = {Nigam, Kamal and McCallum, Andrew and Thrun, Sebastian and Mitchell, Tom},
doi = {citeulike-article-id:1139248},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2000/nigam2000.pdf:pdf},
isbn = {08856125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
number = {2-3},
pages = {103--134},
pmid = {15084654},
title = {{Text Classification from Labeled and Unlabeled Documents using {\{}EM{\}}}},
volume = {39},
year = {2000}
}
@article{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross- validation and bootstrap. Recent experimen- tal results on arti cial data and theoretical re-sults in restricted settings have shown that for selecting a good classi er from a set of classi ers (model selection), ten-fold cross-validation may be better than the more expensive leave- one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the e ects of di erent parameters on these algorithms on real-world datasets. For crossvalidation, we vary the number of folds and whether the folds are strati ed or not for bootstrap, we vary the number of bootstrap sam- ples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold strati ed cross validation, even if computation power allows using more folds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kohavi, Ron},
doi = {10.1067/mod.2000.109031},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/1995/d781305750b37acb35fa187febd8db67bfcc.pdf:pdf},
isbn = {1-55860-363-8},
issn = {10450823},
journal = {Proc. of IJCAI'95},
pages = {1137--1145},
pmid = {11029742},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection 2 Methods for Accuracy Estimation}},
year = {1995}
}
@article{Schwartz2013,
author = {{Thamarai Selvi. S, Karthikeyan. P, Vincent. A, Abinaya. V, Neeraja. G}, Deepika. R},
doi = {10.1016/j.gaitpost.2013.07.085},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2016/Schwartz, Rozumalski, Tom - 2013 - Random Forest algorithm.pdf:pdf},
isbn = {9781509058884},
issn = {09666362},
journal = {IEEE Eighth International Conference on Advanced Computing},
keywords = {and curse of dimensionality,categorization,decision,document is reduced,in the,in this way,information retrieval,problem is,text categorization,the number of features,trees,vector space model},
month = {nov},
pages = {S42--S43},
title = {{Text Categorization using Rocchio Algorithm and Random Forest Algorithm}},
url = {http://dx.doi.org/10.1016/j.gaitpost.2013.07.085 http://linkinghub.elsevier.com/retrieve/pii/S0966636213004086},
volume = {1},
year = {2016}
}
@book{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar},
booktitle = {Online},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2009/Manning, Raghavan - 2009 - An Introduction to Information Retrieval.pdf:pdf},
isbn = {0521865719},
issn = {13864564},
keywords = {keyword},
pmid = {10575050},
publisher = {Cambridge University Press},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
volume = {1},
year = {2009}
}
@article{Klose2000a,
abstract = {In this article we present a prototypical implementation of a software tool for document retrieval which groups/arranges (pre-processed) documents based on a similarity measure. The prototype was developed based on self-organising maps to realise interactive associative search and visual exploration of document databases. This helps a user to navigate through similar documents. The navigation, especially the search for the first appropriate document, is supported by conventional keyword search methods. The usability of the presented approach is shown by a sample search. {\textcopyright} 2000 Elsevier Science Ltd. All rights reserved.},
author = {Klose, A. and N{\"{u}}rnberger, A. and Kruse, R. and Hartmann, G. and Richards, M.},
doi = {10.1016/S1464-1895(00)00100-9},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2000/Klose et al. - 2000 - Interactive text retrieval based on document similarities.pdf:pdf},
isbn = {1464-1895},
issn = {14641895},
journal = {Physics and Chemistry of the Earth, Part A: Solid Earth and Geodesy},
keywords = {0 2000 elsevier science,a sample search,all rights reserved,ltd,presented ap-,proach is shown by,search methods,the usability of the},
number = {8},
pages = {649--654},
title = {{Interactive text retrieval based on document similarities}},
volume = {25},
year = {2000}
}
@article{Baraniuk2007,
abstract = {This lecture note presents a new method to capture and represent compressible signals at a rate significantly below the Nyquist rate. This method, called compressive sensing, employs nonadaptive linear projections that preserve the structure of the signal; the signal is then reconstructed from these projections using an optimization process.},
author = {Baraniuk, R.G.},
doi = {10.1109/MSP.2007.4286571},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2007/Baraniuk - 2007 - Compressive Sensing Lecture Notes.pdf:pdf},
isbn = {1053-5888 VO - 24},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
pmid = {19158952},
title = {{Compressive Sensing [Lecture Notes]}},
volume = {24},
year = {2007}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2008/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@incollection{Braschler2003,
address = {Berlin, Heidelberg},
annote = {Cite this paper as: 
Braschler M., Ripplinger B. (2003) Stemming and Decompounding for German Text Retrieval. In: Sebastiani F. (eds) Advances in Information Retrieval. ECIR 2003. Lecture Notes in Computer Science, vol 2633. Springer, Berlin, Heidelberg},
author = {Braschler, Martin and Ripplinger, B{\"{a}}rbel},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/3-540-36618-0_13},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2003/braschler2003.pdf:pdf},
pages = {177--192},
publisher = {Springer},
title = {{Stemming and Decompounding for German Text Retrieval}},
url = {http://link.springer.com/10.1007/3-540-36618-0{\_}13},
year = {2003}
}
@article{Xue2015,
abstract = {Due to the good performance in computation speed and efficiency, Random Forest (RF) algorithm as a famous integrated learning algorithm has been widely applied in many fields. In addition, because of the rapid development of Internet, text categorization has become the key technology to process and organize large scale documents. It is appealing and important to employ RF algorithm to deal with text documents categorization problem. This paper introduces the details of RF algorithm and assess the text documents categorization model by using RF algorithm.},
author = {Xue, Dashen and Li, Fengxin},
doi = {10.1109/CICT.2015.101},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2015/Xue, Li - 2015 - Research of Text Categorization Model based on Random Forests.pdf:pdf},
isbn = {978-1-4799-6023-1},
journal = {2015 IEEE International Conference on Computational Intelligence {\&} Communication Technology},
keywords = {Algorithm design and analysis,Bagging,Decision Tree,Decision trees,Internet,RF algorithm,Radio frequency,Random Forests,Support vector machine classification,Text Categorization,Text categorization,Training,integrated learning algorithm,large scale documents,learning (artificial intelligence),random forest algorithm,text analysis,text documents categorization model,text documents categorization problem},
pages = {173--176},
title = {{Research of Text Categorization Model based on Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7078689},
year = {2015}
}
@article{Leopold2002,
abstract = {The choice of the kernel function is crucial to most applications of support vector machines. In this paper, however, we show that in the case of text classification, term-frequency transformations have a larger impact on the performance of SVM than the kernel itself. We discuss the role of importance-weights (e.g. document frequency and redundancy), which is not yet fully understood in the light of model complexity and calculation cost, and we show that time consuming lemmatization or stemming can be avoided even when classifying a highly inflectional language like German.},
author = {Leopold, Edda and Kindermann, J{\"{o}}rg},
doi = {10.1023/A:1012491419635},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2002/Leopold, Kindermann - 2002 - Text categorization with support vector machines. How to represent texts in input space.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Kernel functions,Lemmatization,Stemming,Support vector machines,Text classification},
number = {1-3},
pages = {423--444},
title = {{Text categorization with support vector machines. How to represent texts in input space?}},
volume = {46},
year = {2002}
}
@article{Liu2005,
abstract = {Transforming the results of species distribution modelling from probabilities of or suitabilities for species occurrence to presences/absences needs a specific threshold. Even though there are many approaches to determining thresholds, there is no comparative study. In this paper, twelve approaches were compared using two species in Europe and artificial neural networks, and the modelling results were assessed using four indices: sensitivity, specificity, overall prediction success and Cohen's kappa statistic. The results show that prevalence approach, average predicted probability/suitability approach, and three sensitivity-specificity-combined approaches, including sensitivity-specificity sum maximization approach, sensitivity- specificity equality approach and the approach based on the shortest distance to the top-left corner (0,1) in ROC plot, are the good ones. The commonly used kappa maximization approach is not as good as the afore-mentioned ones, and the fixed threshold approach is the worst one. We also recommend using datasets with prevalence of 50{\%} to build models if possible since most optimization criteria might be satisfied or nearly satisfied at the same time, and therefore it's easier to find optimal thresholds in this situation.},
author = {Liu, C and Berry, P M and Dawson, T P and Pearson, R G},
doi = {10.1111/j.0906-7590.2005.03957.x},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2005/Liu{\_}et{\_}al-2005-Ecography.pdf:pdf},
issn = {09067590},
journal = {Ecography},
number = {December 2004},
pages = {385--393},
title = {{Selecting thresholds of ocurrence in the prediction of species distributions}},
volume = {28},
year = {2005}
}
@book{Olson2008,
author = {{Olson, David L.; and Delen}, Dursun},
edition = {1},
publisher = {Springer},
title = {{Advanced Data Mining Techniques}},
year = {2008}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\{}{\&}{\}} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
annote = {L. Breiman, âRandom Forestsâ, Machine Learning, 45(1), 5-32, 2001},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2001/A{\_}1010933404324.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
number = {1},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Forman2003a,
author = {Forman and George},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2003/forman03a{\_}full.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {com,document categorization,gforman,hp,hpl,roc,supervised learning,support vector machines},
pages = {1289--1305},
title = {{An extensive empirical study of feature selection metrics for text classification}},
volume = {3},
year = {2003}
}
@article{Rokach2005,
abstract = {Decision trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and data mining considered the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The paper suggests a unified algorithmic framework for presenting these algorithms and describes the various splitting criteria and pruning methodologies.},
author = {Rokach, L. and Maimon, O.},
doi = {10.1109/TSMCC.2004.843247},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2005/rokach2005.pdf:pdf},
isbn = {1094-6977 VO - 35},
issn = {1094-6977},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
keywords = {Classification,Classification tree analysis,Data mining,Decision trees,Industrial training,Loans and mortgages,Machine learning,Machine learning algorithms,Pattern recognition,Predictive models,Statistics,data mining,decision trees,decision trees classifiers,learning by example,machine learning,pattern classification,pattern recognition,pruning method,pruning methods,regression analysis,splitting criteria,top-down induction,tree searching},
number = {4},
pages = {476--487},
title = {{Top-Down Induction of Decision Trees ClassifiersâA Survey}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1522531},
volume = {35},
year = {2005}
}
@book{Schultz1968,
annote = {Angelehnt an Abbildung, Seite 120},
author = {Schultz, Claire K},
language = {eng},
pages = {120},
publisher = {New York (N.Y.) : Spartan books},
title = {{H. P. Luhn: Pioneer of information science : selected works}},
url = {http://lib.ugent.be/catalog/rug01:000820085},
year = {1968}
}
@article{scikit-learn,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard and Duchesnay, E},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2011/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in {\{}P{\}}ython.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2011}
}
@article{Colas2006,
author = {Colas, Fabrice and Brazdil, Pavel},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2006/Colas, Brazdil - 2006 - On the behavior of SVM and some older algorithms in binary text classification tasks.pdf:pdf},
isbn = {3540390901},
issn = {16113349},
journal = {Text, Speech and Dialogue},
pages = {45--52},
title = {{On the behavior of SVM and some older algorithms in binary text classification tasks}},
volume = {LECTURE NO},
year = {2006}
}
@article{Cohen1999,
abstract = {Two recently implemented machine learning algorithms, RIPPER$\backslash$nand sleeping experts for phrases, are evaluated on a$\backslash$nnumber of large text categorization problems. These algorithms$\backslash$nboth construct classifiers that allow the "context" of$\backslash$na word w to affect how (or even whether) the presence or$\backslash$nabsence of w will contribute to a classification. However,$\backslash$nRIPPER and sleeping experts differ radically in many other$\backslash$nrespects: differences include different notions as to what constitutes$\backslash$na context, different...},
author = {Cohen, William W. and Singer, Yoram},
doi = {10.1145/306686.306688},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/1999/Cohen, Singer - 1999 - Context-sensitive learning methods for text categorization.pdf:pdf},
isbn = {0897917928},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
number = {2},
pages = {141--173},
title = {{Context-sensitive learning methods for text categorization}},
volume = {17},
year = {1999}
}
@article{McCallum1999,
abstract = {In many important document classication tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classication approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to ll in this missing value, learning both the...},
author = {McCallum, Andrew},
doi = {10.1.1.35.888},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/1999/multilabel.pdf:pdf},
journal = {AAAI'99 Workshop on Text Learning},
pages = {1--7},
title = {{Multi-label text classification with a mixture model trained by EM}},
url = {http://www.kyriakides.net/CBCL/references/Papers/mccallum99multilabel.pdf},
year = {1999}
}
@article{Kullarni2013,
abstract = {Random Forest is an ensemble supervised machine learning technique. Machine learning techniques have applications in the area of Data mining. Random Forest has tremendous potential of becoming a popular technique for future classifiers because its performance has been found to be comparable with ensemble techniques bagging and boosting. Hence, an in-depth study of existing work related to Random Forest will help to accelerate research in the field of Machine Learning. This paper presents a systematic survey of work done in Random Forest area. In this process, we derived Taxonomy of Random Forest Classifier which is presented in this paper. We also prepared a Comparison chart of existing Random Forest classifiers on the basis of relevant parameters. The survey results show that there is scope for improvement in accuracy by using different split measures and combining functions; and in performance by dynamically pruning a forest and estimating optimal subset of the forest. There is also scope for evolving other novel ideas for stream data and imbalanced data classification, and for semi-supervised learning. Based on this survey, we finally presented a few future research directions related to Random Forest classifier.},
author = {Kullarni, Vrushali Y and Sinha, Pradeep K},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2013/Kullarni, Sinha - 2013 - Random Forest Classifier A Survey and Future Research Directions.pdf:pdf},
issn = {2051-0845},
journal = {International Journal of Advanced Computing},
keywords = {Review Papers},
number = {1},
pages = {1144--1156},
title = {{Random Forest Classifier: A Survey and Future Research Directions}},
volume = {36},
year = {2013}
}
@article{Keikha2009,
abstract = {There are three factors involved in text classification. These are classification model, similarity measure and document representation model. In this paper, we will focus on document representation and demonstrate that the choice of document representation has a profound impact on the quality of the classifier. In our experiments, we have used the centroid-based text classifier, which is a simple and robust text classification scheme. We will compare four different types of document representations: N-grams, Single terms, phrases and RDR which is a logic-based document representation. The N-gram representation is a string-based representation with no linguistic processing. The Single term approach is based on words with minimum linguistic processing. The phrase approach is based on linguistically formed phrases and single words. The RDR is based on linguistic processing and representing documents as a set of logical predicates. We have experimented with many text collections and we have obtained similar results. Here, we base our arguments on experiments conducted on Reuters-21578. We show that RDR, the more complex representation, produces more effective classifier on Reuters-21578, followed by the phrase approach. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Keikha, Mostafa and Khonsari, Ahmad and Oroumchian, Farhad},
doi = {10.1016/j.knosys.2008.06.002},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2009/Keikha, Khonsari, Oroumchian - 2009 - Rich document representation and classification An analysis.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Ordered weighted average,Rich document representation,Text classification},
number = {1},
pages = {67--71},
publisher = {Elsevier B.V.},
title = {{Rich document representation and classification: An analysis}},
url = {http://dx.doi.org/10.1016/j.knosys.2008.06.002},
volume = {22},
year = {2009}
}
@article{Isa2008,
abstract = {This work implements an enhanced hybrid classification method through the utilization of the naive Bayes approach and the support vector machine (SVM). In this project, the Bayes formula was used to vectorize (as opposed to classify) a document according to a probability distribution reflecting the probable categories that the document may belong to. The Bayes formula gives a range of probabilities to which the document can be assigned according to a predetermined set of topics (categories) such as those found in the "20 Newsgroups" data set for instance. Using this probability distribution as the vectors to represent the document, the SVM can then be used to classify the documents on a multidimensional level. The effects of an inadvertent dimensionality reduction caused by classifying using only the highest probability using the naive Bayes classifier can be overcome using the SVM by employing all the probability values associated with every category for each document. This method can be used for any data set and shows a significant reduction in training time as compared to the Lsquare method and significant improvement in the classification accuracy when compared to pure naive Bayes systems and also the TF-IDF/SVM hybrids.},
author = {Isa, D and Lee, L H and Kallimani, V P and RajKumar, R},
doi = {10.1109/TKDE.2008.76},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2008/Isa et al. - 2008 - Text Document Preprocessing with the Bayes Formula for Classification Using the Support Vector Machine.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {applications {\&}{\#}x00026;{\#}x003c,applications {\&}{\#}x003c,classifier design evaluation {\&}{\#}x00026;{\#}x003c,classifier design evaluation {\&}{\#}x003c,computing methodologies,design methodology {\&}{\#}x00026;{\#}x003c,design methodology {\&}{\#}x003c,document analysis,document text processing {\&}{\#}x00026;{\#}x003c,document text processing {\&}{\#}x003c,pattern recognition {\&}{\#}x00026;{\#}x003c,pattern recognition {\&}{\#}x003c,text processing {\&}{\#}x00026;{\#}x003c,text processing {\&}{\#}x003c},
number = {9},
pages = {1264--1272},
title = {{Text Document Preprocessing with the Bayes Formula for Classification Using the Support Vector Machine}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4492780},
volume = {20},
year = {2008}
}
@article{Zhang2007,
abstract = {Text classification algorithms, such SVM, and Naive Bayes, have been developed to build up search engines and construct spam email filters. As a simple yet powerful sample of Bayesian theorem, naive Bayes shows advantages in text classification yielding satisfactory results. In this paper, a spam email detector is developed using naive Bayes algorithm. We use pre-classified emails (priory knowledge) to train the spam email detector. With the model generated from the training step, the detector is able to decide whether an email is a spam email or an ordinary email.},
author = {Zhang, Haiyi and Li, Di},
doi = {10.1109/GRC.2007.4403192},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2007/zhang2007.pdf:pdf},
isbn = {076953032X},
journal = {Proceedings - 2007 IEEE International Conference on Granular Computing},
pages = {708--711},
title = {{Naive Bayes text classifier}},
year = {2007}
}
@article{Mladenic2004,
abstract = {This paper explores feature scoring and selection based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Na{\"{i}}ve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.},
author = {Mladeni{\'{c}}, Dunja and Brank, Janez and Grobelnik, Marko and {Natasa Milic-Frayling}, Ijssi},
doi = {10.1145/1008992.1009034},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2004/Mladeni{\'{c}} et al. - 2004 - Feature Selection using Linear Classifier Weights Interaction with Classification Models.pdf:pdf},
isbn = {1581138814},
journal = {Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {Experimentation Keywords Text classification,Indexing General Terms Algorithms,Performance,SVM normal,feature scoring,feature selection,information retrieval,linear SVM,vector representation},
pages = {234--241},
title = {{Feature Selection using Linear Classifier Weights: Interaction with Classification Models}},
year = {2004}
}
@article{Oshiro2012,
abstract = {Random Forest is a computationally efficient technique that can operate quickly over large datasets. It has been used in many recent research projects and real-world applications in diverse domains. However, the associated literature provides almost no directions about how many trees should be used to compose a Random Forest. The research reported here analyzes whether there is an optimal number of trees within a Random Forest, i.e., a threshold from which increasing the number of trees would bring no significant performance gain, and would only increase the computational cost. Our main conclusions are: as the number of trees grows, it does not always mean the performance of the forest is significantly better than previous forests (fewer trees), and doubling the number of trees is worthless. It is also possible to state there is a threshold beyond which there is no significant gain, unless a huge computational environment is available. In addition, it was found an experimental relationship for the AUC gain when doubling the number of trees in any forest. Furthermore, as the number of trees grows, the full set of attributes tend to be used within a Random Forest, which may not be interesting in the biomedical domain. Additionally, datasets' density-based metrics proposed here probably capture some aspects of the VC dimension on decision trees and low-density datasets may require large capacity machines whilst the opposite also seems to be true.},
author = {Oshiro, Thais Mayumi and Perez, Pedro Santoro and Baranauskas, Jos{\'{e}} Augusto},
doi = {10.1007/978-3-642-31537-4_13},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2012/oshiro2012.pdf:pdf},
isbn = {9783642315367},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Number of Trees,Random Forest,VC Dimension},
pages = {154--168},
title = {{How many trees in a random forest?}},
volume = {7376 LNAI},
year = {2012}
}
@article{Kim2005,
abstract = {Support vector machines (SVMs) have been recognized as one of the most successful classifica- tion methods for many applications including text classification. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to effi- ciently handle a large number of terms in practical applications of text classification. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problemwhere a documentmay belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced},
author = {Kim, Hyunsoo and Howland, Peg and Park, Haesun},
doi = {10.1021/bi702018v},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2005/Kim, Howland, Park - 2005 - Dimension Reduction in Text Classification with Support Vector Machines.pdf:pdf},
isbn = {0006-2960},
issn = {15337928},
journal = {Journal ofMachine Learning Research},
keywords = {centroids,dimension reduction,linear discriminant analysis,support vector machines,text classification},
pages = {37--53},
pmid = {18154308},
title = {{Dimension Reduction in Text Classification with Support Vector Machines}},
volume = {6},
year = {2005}
}
@inproceedings{Khan10areview,
author = {Khan, Aurangzeb and Baharudin, Baharum and Lee, Lam Hong and Khan, Khairullah and Tronoh, Universiti Teknologi Petronas},
booktitle = {Journal of Advances In Information Technology, VOL},
doi = {10.1.1.475.4569},
title = {{A Review of Machine Learning Algorithms for Text-Documents Classification}},
year = {2010}
}
@article{Mesleh2007,
author = {Mesleh, Ama},
doi = {10.3844/jcssp.2007.430.435},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2007/download.pdf:pdf},
issn = {15493636},
journal = {Journal of Computer Science},
keywords = {arabic text categorization,arabic text classification,chi square feature extraction},
number = {6},
pages = {430--435},
title = {{Chi Square Feature Extraction Based Svms Arabic Language Text Categorization System.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=15493636{\&}AN=25553449{\&}h=0MvMO{\%}2BMdK6FD{\%}2BGb5bsFRyYm4CwV2C{\%}2B4DojbvBazNpdKWm3CeqPHGjacR717u{\%}2BVko2g5c2rnv{\%}2BM7lq4l36{\%}2Bj6jQ{\%}3D{\%}3D{\&}crl=c},
volume = {3},
year = {2007}
}
@article{Aggarwal2012a,
abstract = {The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aggarwal, CharuC. and Zhai, ChengXiang},
doi = {10.1007/978-1-4614-3223-4_6},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2012/Aggarwal, Zhai - 2012 - A Survey of Text Classification Algorithms(2).pdf:pdf},
isbn = {978-1-4614-3222-7},
issn = {1098-6596},
journal = {Mining Text Data},
keywords = {Text Classification},
pages = {163--222},
pmid = {25246403},
title = {{A Survey of Text Classification Algorithms}},
url = {http://dx.doi.org/10.1007/978-1-4614-3223-4{\_}6},
year = {2012}
}
@article{AmariShun1999,
author = {{Amari, Shun-ichi}, Si Wu},
journal = {Neural Networks},
number = {6},
pages = {783--789},
title = {{Improving support vector machine classifiers by modifying kernel functions}},
volume = {12},
year = {1999}
}
@article{ZAMM1962,
author = {Bittner, L},
doi = {10.1002/zamm.19620420718},
issn = {00442267},
journal = {ZAMM - Zeitschrift f{\"{u}}r Angewandte Mathematik und Mechanik},
number = {7-8},
pages = {364--365},
publisher = {WILEY-VCH Verlag},
title = {{R. Bellman, Adaptive Control Processes. A Guided Tour. XVI + 255 S. Princeton, N. J., 1961. Princeton University Press.}},
url = {http://dx.doi.org/10.1002/zamm.19620420718 http://doi.wiley.com/10.1002/zamm.19620420718},
volume = {42},
year = {1962}
}
@article{Hotho2005,
abstract = {The enormous amount of information stored in unstructured texts cannot sim- ply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing meth- ods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and in- terdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, in- formation extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.},
author = {Hotho, Andreas and N{\"{u}}rnberger, Andreas and Paa{\ss}, Gerhard},
doi = {10.1111/j.1365-2621.1978.tb09773.x},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2005/hotho05TextMining.pdf:pdf},
isbn = {0175-1336},
issn = {01751336},
journal = {LDV Forum - GLDV Journal for Computational Linguistics and Language Technology},
pages = {19--62},
title = {{A Brief Survey of Text Mining}},
url = {http://www.kde.cs.uni-kassel.de/hotho/pub/2005/hotho05TextMining.pdf},
volume = {20},
year = {2005}
}
@article{Manel2001,
abstract = {1)Models for predicting the distribution of organisms from environmental data are widespread in ecology and conservation biology.Their prformance is invariably evaluated from the {\%} success at predicting occurence at test locations 20Using logistic regression with real data from 34 families of aquatic invertebrates in 180 Himmalayan streams, we will illustrate how this widespread measure of predictive accuracy is affected systematically by the prevalence of the target organism 3)With the same invertebrates , we examined alternative perfomance measures used in remote sensing and medical diagnosis.We particularily explored receiveroperating charactoristic(ROC) plots ,from which we derived i)the area under each curve (AUC),considered an effective indicator of model performance independent of the threshold that maximises the {\%} of the true absence and presence that are correctly identified. 4)AUC measures from the ROC plots were independent of prevelence, but highly significantly corelated with much more easily computed kappa.moreover , when applied in predictive mode to test data , models with thresholds optimized by ROC errorneously overestimated true occurences among scarcer organisms,often those of greatest conservation interest. 5)Our strongest recommentation is that ecologist reduce their reliance on prediction success as a perfomance measure in the presents -absence modelling. None of the of the performance measures we examined test the statistical significance accuracy, and we identify this as a priority area for research and development.},
author = {Manel, S and Williams, H C and Ormerod, S J},
doi = {10.1046/j.1365-2664.2001.00647.x},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2001/Manel{\_}et{\_}al-2001-Journal{\_}of{\_}Applied{\_}Ecology.pdf:pdf},
isbn = {1365-2664},
issn = {00218901},
journal = {Journal of Appied ecology},
keywords = {Cohen`s kappa,ROC,logistic regession,model performance,model testing,species,validation},
pages = {921--931},
pmid = {6778},
title = {{Evaluating presence absence models in ecology; the need to count for prevalence}},
volume = {38},
year = {2001}
}
@article{Wu2014,
abstract = {In this paper, we propose a new random forest (RF) based ensemble method, ForesTexter, to solve the imbalanced text categorization problems. RF has shown great success in many real-world applications. However, the problem of learning from text data with class imbalance is a relatively new challenge that needs to be addressed. A RF algorithm tends to use a simple random sampling of features in building their decision trees. As a result, it selects many subspaces that contain few, if any, informative features for the minority class. Furthermore, the Gini measure for data splitting is considered to be skew sensitive and bias towards the majority class. Due to the inherent complex characteristics of imbalanced text datasets, learning RF from such data requires new approaches to overcome challenges related to feature subspace selection and cut-point choice while performing node splitting. To this end, we propose a new tree induction method that selects splits, both feature subspace selection and splitting criterion, for RF on imbalanced text data. The key idea is to stratify features into two groups and to generate effective term weighting for the features. One group contains positive features for the minority class and the other one contains the negative features for the majority class. Then, for feature subspace selection, we effectively select features from each group based on the term weights. The advantage of our approach is that each subspace contains adequate informative features for both minority and majority classes. One difference between our proposed tree induction method and the classical RF method is that our method uses Support Vector Machines (SVM) classifier to split the training data into smaller and more balance subsets at each tree node, and then successively retrains the SVM classifiers on the data partitions to refine the model while moving down the tree. In this way, we force the classifiers to learn from refined feature subspaces and data subsets to fit the imbalanced data better. Hence, the tree model becomes more robust for text categorization task with imbalanced dataset. Experimental results on various benchmark imbalanced text datasets (Reuters-21578, Ohsumed, and imbalanced 20 newsgroup) consistently demonstrate the effectiveness of our proposed ForesTexter method. The performance of our proposed approach is competitive against the standard random forest and different variants of SVM algorithms. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Wu, Qingyao and Ye, Yunming and Zhang, Haijun and Ng, Michael K. and Ho, Shen Shyang},
doi = {10.1016/j.knosys.2014.06.004},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2014/Wu et al. - 2014 - ForesTexter An efficient random forest algorithm for imbalanced text categorization.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Imbalanced classification,Random forests,SVM,Stratified sampling,Text categorization},
pages = {105--116},
publisher = {Elsevier B.V.},
title = {{ForesTexter: An efficient random forest algorithm for imbalanced text categorization}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.06.004},
volume = {67},
year = {2014}
}
@article{Yang1997,
abstract = {This paper is a comparative study of feature selection methods in statistical learning of text categorization. The focus is on aggres- sive dimensionality reduction. Five meth- ods were evaluated, including term selection based on document frequency (DF), informa- tion gain (IG), mutual information (MI), a 2-test (CHI), and term strength (TS). We found IG and CHI most e ective in our ex- periments. Using IG thresholding with a k- nearest neighbor classi er on the Reuters cor- pus, removal of up to 98{\{}{\%}{\}} removal of unique terms actually yielded an improved classi - cation accuracy (measured by average preci- sion). DF thresholding performed similarly. Indeed we found strong correlations between the DF, IG and CHI values of a term. This suggests that DF thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of IG or CHI when the computation of these measures are too expensive. TS compares favorably with the other methods with up to 50{\{}{\%}{\}} vocabulary reduction but is not competitive at higher vo- cabulary reduction levels. In contrast, MI had relatively poor performance due to its bias towards favoring rare terms, and its sen- sitivity to probability estimation errors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yang, Yiming and Pedersen, Jan O.},
doi = {10.1093/bioinformatics/bth267},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/1997/Yang, Pedersen - 1997 - A Comparative Study on Feature Selection in Text Categorization.pdf:pdf},
isbn = {1-55860-486-3},
issn = {1976-3700},
journal = {Proceedings of the Fourteenth International Conference on Machine Learning (ICML'97)},
pages = {412--420},
pmid = {15087314},
title = {{A Comparative Study on Feature Selection in Text Categorization}},
year = {1997}
}
@article{Li2004,
abstract = {This paper is a comparative study of feature selection methods in$\backslash$nstatistical learning of text categorization . The focus is on aggres-$\backslash$nsive dimensionality reduction. Five meth- ods were evaluated, including$\backslash$nterm selection based on document frequency (DF), informa- tion gain$\backslash$n...},
author = {Li, T. and Zhang, C. and Ogihara, M.},
doi = {10.1093/bioinformatics/bth267},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2004/Li, Zhang, Ogihara - 2004 - A comparative study of feature selection and multiclass classification methods for tissue classification bas.pdf:pdf},
isbn = {1558604863},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {2429--2437},
pmid = {15087314},
title = {{A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bth267},
volume = {20},
year = {2004}
}
@article{Sebastiani2001,
abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.},
archivePrefix = {arXiv},
arxivId = {cs/0110053},
author = {Sebastiani, Fabrizio},
doi = {10.1145/505282.505283},
eprint = {0110053},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2001/Sebastiani - 2001 - Machine Learning in Automated Text Categorization.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {mar},
number = {1},
pages = {1--47},
pmid = {18202440},
primaryClass = {cs},
title = {{Machine learning in automated text categorization}},
url = {http://arxiv.org/abs/cs/0110053 http://portal.acm.org/citation.cfm?doid=505282.505283},
volume = {34},
year = {2002}
}
@article{Rokach2009,
abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving predic- tion performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
author = {Rokach, Lior},
doi = {10.1007/s10462-009-9124-7},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2010/rokach2009.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Boosting,Classification,Ensemble of classifiers,Supervised learning},
month = {feb},
pages = {1--39},
title = {{Ensemble-based classifiers}},
url = {http://link.springer.com/10.1007/s10462-009-9124-7},
volume = {33},
year = {2010}
}
@article{McCallum1998a,
abstract = {Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizesâproviding on average a 27{\%} reduction in error over the multi-variate Bernoulli model at any vocabulary size. Introduction},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {McCallum, Andres and Nigam, Kamal},
doi = {10.1.1.46.1529},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/1998/McCallum, Nigam - 1998 - A Comparison of Event Models for Naive Bayes Text Classification.pdf:pdf},
isbn = {0897915240},
issn = {0343-6993},
journal = {AAAI/ICML-98 Workshop on Learning for Text Categorization},
pages = {41--48},
pmid = {20236947},
title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@book{WittenX2011,
author = {{Witten, Ian; Frank, Eibe; Hall}, Mark},
doi = {10.1145/2020976.2021004},
issn = {01635948},
keywords = {Burlington,MA},
mendeley-tags = {Burlington,MA},
month = {sep},
pages = {102--103},
publisher = {Morgan Kaufmann},
title = {{Data mining: practical machine learning tools and technique}},
url = {http://dl.acm.org/citation.cfm?doid=2020976.2021004},
year = {2011}
}
@article{Liu2017,
abstract = {Extreme multi-label text classiication (XMTC) refers to the prob-lem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. huge label space raises research challenges such as data sparsity and scalability. Signiicant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. paper presents the aaempt at applying deep learning to XMTC, with a family of new Convolutional Neural Net-work (CNN) models which are tailored for multi-label classiication in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in partic-ular, it outperformed the second best method by 11.7{\%} â¼ 15.3{\%} in precision@K and by 11.5{\%} â¼ 11.7{\%} in NDCG@K for K = 1,3,5.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Liu, Jingzhou and Chang, Wei-Cheng and Wu, Yuexin and Yang, Yiming},
doi = {10.1145/3077136.3080834},
eprint = {arXiv:1602.05561v1},
file = {:C$\backslash$:/Users/Chris Djih/Documents/BA/BA{\_}Thesis/NEW/Literatur/BibTex/2017/liu2017.pdf:pdf},
isbn = {9781450350228},
issn = {16130073},
journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval  - SIGIR '17},
pages = {115--124},
title = {{Deep Learning for Extreme Multi-label Text Classification}},
url = {http://dl.acm.org/citation.cfm?doid=3077136.3080834},
year = {2017}
}
